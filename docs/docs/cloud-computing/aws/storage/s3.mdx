---
title: S3
description: S3
keywords:
  - S3
sidebar_position: 2
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

## Behaviour 


### Strongly consistent

All S3 GET, PUT, and LIST operations, as well as operations that change object tags, ACLs, or metadata, are **strongly consistent**. What you write is what you will read, and the results of a LIST will be an accurate reflection of what’s in the bucket.

### Lifecycle

You can add rules in an S3 Lifecycle configuration to tell Amazon S3 to transition objects to another Amazon S3 Using Amazon S3 storage classes.Amazon S3 supports a waterfall model for transitioning between storage classes, as shown in the following diagram.

![lifecycle-transitions](/img/aws/networking/lifecycle-transitions.png)
Source: [Transitioning objects using Amazon S3 Lifecycle](https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html)

## Features

### S3 Select

[S3 Select](https://aws.amazon.com/s3/details/#s3-select), enables applications to retrieve only a subset of data from an object by using simple SQL expressions. By using S3 Select to retrieve only the data needed by your application, you can achieve drastic performance increases -- in many cases you can get as much as a 400% improvement.

![](https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/28/s3_select.png)

As an example, let's imagine you're a developer at a large retailer and you need to analyze the weekly sales data from a single store, but the data for all 200 stores is saved in a new GZIP-ed CSV every day. **Without S3 Select, you would need to download, decompress and process the entire CSV to get the data you needed.** With S3 Select, you can use a simple SQL expression to return only the data from the store you're interested in, instead of retrieving the entire object. This means you're dealing with an order of magnitude less data which improves the performance of your underlying applications.

Let's look at a quick Python example, which shows how to retrieve the first column from an object containing data in CSV format.

```
import boto3
s3 = boto3.client('s3')

r = s3.select_object_content(
        Bucket='jbarr-us-west-2',
        Key='sample-data/airportCodes.csv',
        ExpressionType='SQL',
        Expression="select * from s3object s where s.\"Country (Name)\" like '%United States%'",
        InputSerialization = {'CSV': {"FileHeaderInfo": "Use"}},
        OutputSerialization = {'CSV': {}},
)

for event in r['Payload']:
    if 'Records' in event:
        records = event['Records']['Payload'].decode('utf-8')
        print(records)
    elif 'Stats' in event:
        statsDetails = event['Stats']['Details']
        print("Stats details bytesScanned: ")
        print(statsDetails['BytesScanned'])
        print("Stats details bytesProcessed: ")
        print(statsDetails['BytesProcessed'])
```

We expect customers to use S3 Select to accelerate all sorts of applications. For example, this partial data retrieval ability is especially useful for serverless applications built with [AWS Lambda](https://aws.amazon.com/lambda/). When we modified the [Serverless MapReduce](https://github.com/awslabs/lambda-refarch-mapreduce) reference architecture to retrieve only the data needed using S3 Select we saw a 2X improvement in performance and an 80% reduction in cost.

:::info Things to know
[Amazon Athena](https://aws.amazon.com/athena), [Amazon Redshift](https://aws.amazon.com/redshift/), and [Amazon EMR](https://aws.amazon.com/emr) as well as partners like Cloudera, DataBricks, and Hortonworks will all support S3 Select.
:::

### Notification feature 

AWS notification feature enables you to receive notifications when certain events happen in your bucket. Amazon S3 supports the following destinations where it can publish events:
- Amazon Simple Notification Service (Amazon SNS) topic
- Amazon Simple Queue Service (Amazon SQS) queue
- AWS Lambda

## Performance


### Byte-Range Fetches

Byte-Range Fetches is an option that allows you to read a file in the most efficient way. Using the Range HTTP header in a [GET Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html) request, you can fetch **a byte-range from an object**, transferring only the specified portion. You can use concurrent connections to Amazon S3 to fetch different byte ranges from within the same object. 

- This helps you achieve higher aggregate throughput versus a single whole-object request.
- Fetching smaller ranges of a large object also allows your application to improve retry times when requests are interrupted. For more information, see [Getting Objects](https://docs.aws.amazon.com/AmazonS3/latest/dev/GettingObjectsUsingAPIs.html).

Typical sizes for byte-range requests are 8 MB or 16 MB. If objects are PUT using a multipart upload, it's a good practice to GET them in the same part sizes (or at least aligned to part boundaries) for best performance. GET requests can directly address individual parts; for example, `GET ?partNumber=N`. 

You can implement byte-range fetches in 2 different ways **Parallelize GETs** and **retrieve only partial data**. 

<Tabs className="api-tabs">
<TabItem value="before" label="Parallelize GETs">

It is to parallelize GETs by requesting a specific byte range. In case of failure, it has better resilience. Hence, it could be used to speed up downloads.

![fetch-header](/img/aws/storage/s3/fetch-header.png)
Source: [Optimize Your AWS S3 Performance](https://aws.plainenglish.io/optimize-your-aws-s3-performance-27b057f231a3)

</TabItem>
<TabItem value="after" label="Retrieve only partial data">

The second use-case in which S3 Byte-Range Fetches could be used is to retrieve only partial data. For example, when you know the first XX bytes is the header of a file, in this case, it could be used.

![fetch-parallel](/img/aws/storage/s3/fetch-parallel.png)
Source: [Optimize Your AWS S3 Performance](https://aws.plainenglish.io/optimize-your-aws-s3-performance-27b057f231a3)

</TabItem>
</Tabs>

```bash
# S3 Byte-Range Fetches via cURL

$ curl -r -1024 https://s3.amazonaws.com/mybucket/myobject -o parti 
$ curl -r 1025- https://s3.amazonaws.com/mybucket/myobject -o part2 
$ cat parti part2 > myobject
```


```javascript
// S3 Byte-Range Fetches with AWS SDK

var s3 = new AWS.530); 
var file = require('fs').createWriteStream('parti'); 
var params = {
  Bucket: 'my bucket', 
  Key: 'myobject', 
  Range: 'bytes=-1024
};

s3.getObject(params).createReadStream().pipe(file);
```

### Read Replicas

Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. **Read replicas can be within an Availability Zone, Cross-AZ, or Cross-Region.**

### Upload and download speed 

There are 2 approaches to improve upload and download speed across different regions. Depending on your setup, S3 Transfer Acceleration [suggested in another answer](https://stackoverflow.com/a/55148734/1081234) may still be a better option, it's certainly better documented and has more implementation examples!

In terms of AWS pricing, you will be charged for "[Regional Data Transfer Out to Origin](https://aws.amazon.com/cloudfront/pricing/)" ($0.02-$0.16/GB depending on region) compared with [S3 Transfer Acceleration](https://aws.amazon.com/s3/pricing/) charges ($0.04-$0.08/GB depending on region.)

<Tabs className="upload-download-tabs">
<TabItem value="s3-acceleration" label="S3 Transfer Acceleration">

Amazon S3 Transfer Acceleration can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path. 

</TabItem>
<TabItem value="cloudfront" label="CloudFront distribution">

Use Amazon CloudFront distribution with origin as the S3 bucket to ingest S3 file uploads.

- **Upload** - `PUT` and `POST` operations will be sent to the origin(e.g. Amazon S3).  For a deep-dive reference, visit [AWS document](https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/)
- **Download** - When an object from S3 that is set up with CloudFront CDN is requested, the request would come through the **Edge Location** transfer paths only for the first request. Thereafter, it would be served from the nearest edge location to the users until it expires. 

> Belows are the high level steps to create a cloudfront distribution. To check the detailed steps, visit [StackOverflow - Uploads to S3 through CloudFront via Signed URLs?](https://stackoverflow.com/questions/55148428/uploads-to-s3-through-cloudfront-via-signed-urls)

1. Create a new CF distribution
  -   Use target S3 bucket as origin
  -   Configure origin S3 bucket access to use a CloudFront origin access identity. While you can automatically update the bucket policy (with `s3:GetObject` access) as a starting point, we'll be adding/changing that to `s3:PutObject` anyway.
2. Configure S3 bucket policy
3. Upload files to S3 using the CloudFront distribution

  Once the distribution is updated, you would be able to add files to your bucket by making a `PUT` request to `https://your-distribution-url/desired-s3-key-name` using Postman or something. Remember to use a signed url here if that's what you've configured.
  Your client connections to the CF edge should be consistently faster, while anecdotally S3 acceleration [speed comparison](http://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html) isn't always favorable.

</TabItem>
</Tabs>

## Availabiltiy

### Multi-AZ

Multi-AZ for the RDS database helps make our database highly-available, but the standby database is not accessible and cannot be used for reads or write. It's just a database that will become primary when the other database encounters a failure.


## Limitation

### Request per sec
Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Your applications can easily achieve thousands of transactions per second in request performance when uploading and retrieving storage from Amazon S3. Amazon S3 automatically scales to high request rates. 

For example, your application can achieve **at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET requests per second per prefix in a bucket**.

There are no limits to the number of prefixes in a bucket. You can increase your read or write performance by parallelizing reads. For example, if you create **10 prefixes** in an Amazon S3 bucket to parallelize reads, you could scale your read performance to **55,000 read requests per second**(10*5,500 GET/HEAD).

For many purposes, it's often enough for applications to mitigate `503` SlowDown errors without having to randomize prefixes.

However, if the the limits are not sufficient, prefixes would need to be used. A prefix has no fixed number of characters. It is any string between a bucket name and an object name, for example:

- bucket/folder1/sub1/file
- bucket/folder1/sub2/file
- bucket/1/file
- bucket/2/file

Prefixes of the object 'file' would be: `/folder1/sub1/` , `/folder1/sub2/`, `/1/`, `/2/`. In this example, if you spread reads across all four prefixes evenly, you can achieve **22,000 requests per second.**

### Glacier Vault

You can't move data directly from Snowball into a **Glacier Vault** or a **Glacier Deep Archive Vault**. You need to go through S3 first and then use a lifecycle policy.
## What is prefix

In Amazon S3, you can use prefixes to organize your storage. A prefix is a logical grouping of the objects in a bucket. The prefix value is similar to a directory name that enables you to store similar data under the same directory in a bucket. When you programmatically upload objects, you can use prefixes to organize your data.

In the Amazon S3 console, prefixes are called folders. You can view all your objects and folders in the S3 console by navigating to a bucket. You can also view information about each object, including object properties.

## Further reading

- [Best practices design patterns: optimizing Amazon S3 performance](https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html)
- [Stackoverflow: S3 - What Exactly Is A Prefix? And what Ratelimits apply?](https://stackoverflow.com/questions/52443839/s3-what-exactly-is-a-prefix-and-what-ratelimits-apply)