---
title: S3
description: S3
keywords:
  - S3
sidebar_position: 3
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';


## Basic

### What is prefix

In Amazon S3, you can use prefixes to organize your storage. A prefix is a logical grouping of the objects in a bucket. The prefix value is similar to a directory name that enables you to store similar data under the same directory in a bucket. When you programmatically upload objects, you can use prefixes to organize your data.

In the Amazon S3 console, prefixes are called folders. You can view all your objects and folders in the S3 console by navigating to a bucket. You can also view information about each object, including object properties.


### Storage Types

- **S3 Intelligent-Tiering** storage is designed to optimize costs by automatically moving data to the most cost-effective access tier, without performance impact or operational overhead.

- **S3 Standard-IA** is for data that is accessed less frequently but requires rapid access when needed. (high durability, high throughput, and low latency). 

:::caution Use Standard-IA only when you need big thoughput
If you need to access objects frequently and you don't need a big thoughput, don't use `standard-IA`. Because it will cost you a lot of money.
:::



## Behaviour 

### Strongly consistent

All S3 GET, PUT, and LIST operations, as well as operations that change object tags, ACLs, or metadata, are **strongly consistent**. What you write is what you will read, and the results of a LIST will be an accurate reflection of what’s in the bucket.

### Lifecycle

You can add rules in an S3 Lifecycle configuration to tell Amazon S3 to transition objects to another Amazon S3 Using Amazon S3 storage classes.Amazon S3 supports a waterfall model for transitioning between storage classes, as shown in the following diagram.

![lifecycle-transitions](/img/aws/storage/s3/lifecycle-transitions.png)
Source: [Transitioning objects using Amazon S3 Lifecycle](https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html)

### Deleting a bucket is eventually consistent 

If you delete a bucket and immediately list all buckets, the deleted bucket might still appear in the list - *Bucket configurations have an eventual **consistency model** *. If you delete a bucket and immediately list all buckets, the deleted bucket might still appear in the list.

> Details: [Logging requests using server access logging](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html)

:::danger Never push server access logs about a bucket into the same bucket 
If you configured your server access logs this way, then there would be **an infinite loop of logs**. This is because when you write a log file to a bucket, the bucket is also accessed, which then generates another log. A log file would be generated for every log written to the bucket, which creates a loop. **This would create many logs and increase your storage costs**.

Source: [Can I push server access logs about an Amazon S3 bucket into the same bucket?](https://aws.amazon.com/premiumsupport/knowledge-center/s3-server-access-logs-same-bucket/)
:::

## Features


### Pre-signed URL max validation time

You create a pre-signed URL for your object, you must provide your security credentials, specify a bucket name, an object key, specify the HTTP method (GET to download the object) and expiration date and time. The pre-signed URLs are valid only for the specified duration. Valid up to **6 hours**.

### Server access logging

Server access logging provides detailed records for the requests that are made to a bucket. Server access logs are useful for many applications. For example, access log information can be useful in security and access audits. It can also help you learn about your customer base and understand your Amazon S3 bill.



### S3 Select

[S3 Select](https://aws.amazon.com/s3/details/#s3-select), enables applications to retrieve only a subset of data from an object by using simple SQL expressions. By using S3 Select to retrieve only the data needed by your application, you can achieve drastic performance increases -- in many cases you can get as much as a 400% improvement.

![](https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2017/11/28/s3_select.png)

As an example, let's imagine you're a developer at a large retailer and you need to analyze the weekly sales data from a single store, but the data for all 200 stores is saved in a new GZIP-ed CSV every day. **Without S3 Select, you would need to download, decompress and process the entire CSV to get the data you needed.** With S3 Select, you can use a simple SQL expression to return only the data from the store you're interested in, instead of retrieving the entire object. This means you're dealing with an order of magnitude less data which improves the performance of your underlying applications.

Let's look at a quick Python example, which shows how to retrieve the first column from an object containing data in CSV format.

```
import boto3
s3 = boto3.client('s3')

r = s3.select_object_content(
        Bucket='jbarr-us-west-2',
        Key='sample-data/airportCodes.csv',
        ExpressionType='SQL',
        Expression="select * from s3object s where s.\"Country (Name)\" like '%United States%'",
        InputSerialization = {'CSV': {"FileHeaderInfo": "Use"}},
        OutputSerialization = {'CSV': {}},
)

for event in r['Payload']:
    if 'Records' in event:
        records = event['Records']['Payload'].decode('utf-8')
        print(records)
    elif 'Stats' in event:
        statsDetails = event['Stats']['Details']
        print("Stats details bytesScanned: ")
        print(statsDetails['BytesScanned'])
        print("Stats details bytesProcessed: ")
        print(statsDetails['BytesProcessed'])
```

We expect customers to use S3 Select to accelerate all sorts of applications. For example, this partial data retrieval ability is especially useful for serverless applications built with [AWS Lambda](https://aws.amazon.com/lambda/). When we modified the [Serverless MapReduce](https://github.com/awslabs/lambda-refarch-mapreduce) reference architecture to retrieve only the data needed using S3 Select we saw a 2X improvement in performance and an 80% reduction in cost.

:::info Things to know
[Amazon Athena](https://aws.amazon.com/athena), [Amazon Redshift](https://aws.amazon.com/redshift/), and [Amazon EMR](https://aws.amazon.com/emr) as well as partners like Cloudera, DataBricks, and Hortonworks will all support S3 Select.
:::

### Notification feature 

AWS notification feature enables you to receive notifications when certain events happen in your bucket. Amazon S3 supports the following destinations where it can publish events:
- SNS topic
- Standard SQS queue (FIFO SQS queue is not allowed)
- AWS Lambda

:::info Events List
- New object-created events
- Object removal events
- Restore object events
- Reduced Redundancy Storage (RRS) object lost events 
- Replication events
:::

:::caution 2 Writes are made to a single non-versioned object
If two writes are made to a single non-versioned object at the same time, it is **possible that only a single event notification** will be sent. If you want to ensure that an event notification is sent for every successful write, you can enable versioning on your bucket. With versioning, every successful write will create a new version of your object and will also send an event notification.
:::

> Document: [Event notification types and destinations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/notification-how-to-event-types-and-destinations.html)

### Configure bucket as a static website

When you configure your bucket as a static website, the website is available at the AWS Region-specific website endpoint of the bucket.

Depending on your Region, your Amazon S3 website endpoints follow one of these two formats.
- s3-website dash (-) Region
  - http://bucket-name.s3-website.Region.amazonaws.com
- s3-website dot (.) Region
  - http://bucket-name.s3-website-Region.amazonaws.com

These URLs return the default index document that you configure for the website.

## Performance


### Byte-Range Fetches

Byte-Range Fetches is an option that allows you to read a file in the most efficient way. Using the Range HTTP header in a [GET Object](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectGET.html) request, you can fetch **a byte-range from an object**, transferring only the specified portion. You can use concurrent connections to Amazon S3 to fetch different byte ranges from within the same object. 

- This helps you achieve higher aggregate throughput versus a single whole-object request.
- Fetching smaller ranges of a large object also allows your application to improve retry times when requests are interrupted. For more information, see [Getting Objects](https://docs.aws.amazon.com/AmazonS3/latest/dev/GettingObjectsUsingAPIs.html).

Typical sizes for byte-range requests are 8 MB or 16 MB. If objects are PUT using a multipart upload, it's a good practice to GET them in the same part sizes (or at least aligned to part boundaries) for best performance. GET requests can directly address individual parts; for example, `GET ?partNumber=N`. 

You can implement byte-range fetches in 2 different ways **Parallelize GETs** and **retrieve only partial data**. 

<Tabs className="api-tabs">
<TabItem value="before" label="Parallelize GETs">

It is to parallelize GETs by requesting a specific byte range. In case of failure, it has better resilience. Hence, it could be used to speed up downloads.

![fetch-header](/img/aws/storage/s3/fetch-header.png)
Source: [Optimize Your AWS S3 Performance](https://aws.plainenglish.io/optimize-your-aws-s3-performance-27b057f231a3)

</TabItem>
<TabItem value="after" label="Retrieve only partial data">

The second use-case in which S3 Byte-Range Fetches could be used is to retrieve only partial data. For example, when you know the first XX bytes is the header of a file, in this case, it could be used.

![fetch-parallel](/img/aws/storage/s3/fetch-parallel.png)
Source: [Optimize Your AWS S3 Performance](https://aws.plainenglish.io/optimize-your-aws-s3-performance-27b057f231a3)

</TabItem>
</Tabs>

```bash
# S3 Byte-Range Fetches via cURL

$ curl -r -1024 https://s3.amazonaws.com/mybucket/myobject -o parti 
$ curl -r 1025- https://s3.amazonaws.com/mybucket/myobject -o part2 
$ cat parti part2 > myobject
```


```javascript
// S3 Byte-Range Fetches with AWS SDK

var s3 = new AWS.530); 
var file = require('fs').createWriteStream('parti'); 
var params = {
  Bucket: 'my bucket', 
  Key: 'myobject', 
  Range: 'bytes=-1024
};

s3.getObject(params).createReadStream().pipe(file);
```

### Read Replicas

Amazon RDS Read Replicas provide enhanced performance and durability for RDS database (DB) instances. They make it easy to elastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. **Read replicas can be within an Availability Zone, Cross-AZ, or Cross-Region.**


### When to use multipart uploading?

> via - https://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html

In general, **when your object size reaches 100 MB**, you should consider using multipart uploads instead of uploading the object in a single operation.

-   If you're uploading large objects over a stable high-bandwidth network, use multipart uploading to maximize the use of your available bandwidth by uploading object parts in parallel for multi-threaded performance.
-   If you're uploading over a spotty network, use multipart uploading to increase resiliency to network errors by avoiding upload restarts.

### Upload and download speed 

There are 2 approaches to improve upload and download speed across different regions. Depending on your setup, S3 Transfer Acceleration [suggested in another answer](https://stackoverflow.com/a/55148734/1081234) may still be a better option, it's certainly better documented and has more implementation examples!

In terms of AWS pricing, you will be charged for "[Regional Data Transfer Out to Origin](https://aws.amazon.com/cloudfront/pricing/)" ($0.02-$0.16/GB depending on region) compared with [S3 Transfer Acceleration](https://aws.amazon.com/s3/pricing/) charges ($0.04-$0.08/GB depending on region.)

:::info How should I choose between S3 Transfer Acceleration and Amazon CloudFront’s PUT/POST for optimisating upload?

S3 Transfer Acceleration optimizes the TCP protocol and adds additional intelligence between the client and the S3 bucket, making S3 Transfer Acceleration a better choice if a higher throughput is desired. *If you have **objects that are smaller than 1 GB or if the data set is less than 1 GB in size**, you should consider using **Amazon CloudFront**'s PUT/POST commands for optimal performance.*

If you has data larger than 1GB, then **S3 Transfer Acceleration** is a better option.

:::

<Tabs className="upload-download-tabs">
<TabItem value="s3-acceleration" label="S3 Transfer Acceleration">

Amazon S3 Transfer Acceleration can speed up content transfers to and from Amazon S3 by as much as 50-500% for long-distance transfer of larger objects. Transfer Acceleration takes advantage of Amazon CloudFront’s globally distributed edge locations. As the data arrives at an edge location, data is routed to Amazon S3 over an optimized network path. 

</TabItem>
<TabItem value="cloudfront" label="CloudFront distribution">

> Plain English - When an object from **S3** that is set up with CloudFront CDN is requested, the request would come through **the Edge Location** transfer paths only for the first request. Thereafter, it would be served from the **nearest edge location** to the users until it expires. So in this way, you can *speed up uploads as well as downloads for the video files*.


Use Amazon CloudFront distribution with origin as the S3 bucket to ingest S3 file uploads.

- **Upload** - `PUT` and `POST` operations will be sent to the origin(e.g. Amazon S3).  For a deep-dive reference, visit [AWS document](https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/)
- **Download** - When an object from S3 that is set up with CloudFront CDN is requested, the request would come through the **Edge Location** transfer paths only for the first request. Thereafter, it would be served from the nearest edge location to the users until it expires. 

> Belows are the high level steps to create a cloudfront distribution. To check the detailed steps, visit [StackOverflow - Uploads to S3 through CloudFront via Signed URLs?](https://stackoverflow.com/questions/55148428/uploads-to-s3-through-cloudfront-via-signed-urls)

1. Create a new CF distribution
  -   Use target S3 bucket as origin
  -   Configure origin S3 bucket access to use a CloudFront origin access identity. While you can automatically update the bucket policy (with `s3:GetObject` access) as a starting point, we'll be adding/changing that to `s3:PutObject` anyway.
2. Configure S3 bucket policy
3. Upload files to S3 using the CloudFront distribution

  Once the distribution is updated, you would be able to add files to your bucket by making a `PUT` request to `https://your-distribution-url/desired-s3-key-name` using Postman or something. Remember to use a signed url here if that's what you've configured.
  Your client connections to the CF edge should be consistently faster, while anecdotally S3 acceleration [speed comparison](http://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html) isn't always favorable.

</TabItem>
</Tabs>


## Security

### User policies vs ACL vs Bucket Policies

![AWS-S3-access-control-tools](/img/aws/storage/s3/AWS-S3-access-control-tools.png)

Source: [Background: Amazon S3 Access Control Tools](https://www.msp360.com/resources/blog/s3-access-control-tools/)

- [User Policies](https://docs.aws.amazon.com/AmazonS3/latest/dev/example-policies-s3.html): Use the AWS IAM policy syntax to grant access to IAM users in your account.
- [Bucket Policies](https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html): Use the AWS IAM policy syntax to manage access for a particular S3 bucket and the objects in it; (Sai: A bucket policy is a resource-based policy - [Unlike an identity-based policy, a resource-based policy specifies who (which principal) can access that resource](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compare-resource-policies.html).)
- [Access Control Lists (ACLs)](https://docs.aws.amazon.com/AmazonS3/latest/dev/S3_ACLs_UsingACLs.html): Use XML syntax to grant access to specific S3 buckets or objects.


<Tabs className="UserPolicies-ACL-BucketPolicies">
<TabItem value="user" label="User Policies">

**User policies** are attached to a particular IAM user to indicate whether that user can access various S3 buckets and objects. 

In contrast, **bucket policies** and **ACLs** are attached to the resource itself *either an S3 bucket or an S3 object* to control access.
IAM roles and resource-based policies delegate access across accounts only within a single partition. 

For example, you can't add cross-account access between an account in the standard aws partition and an account in the aws-cn partition. [(reference)](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_compare-resource-policies.html)

</TabItem>
<TabItem value="acl" label="ACL">

Diagram to show S3 Access Polices (Bucket and Object)

![s3-policy-diagram](/img/aws/storage/s3/s3-policy-diagram.png)

Source: [Sample S3 Bucket Policies — Part 01](https://crishantha.medium.com/sample-s3-bucket-policies-part-01-dcdd62772da5)


ACL in AWS console
![s3-policy-diagram](/img/aws/storage/s3/ACL-permission-console.png)

Source: [An easier way to control access to aws resources by using the aws organization of iam principals](https://noise.getoto.net/tag/s3-bucket-policy/)


</TabItem>
<TabItem value="bucketpolicies" label="BucketPolicies">

The following example policy grants the `s3:PutObject` and `s3:PutObjectAcl` permissions to multiple AWS accounts and requires that any request for these operations include the `public-read` canned access control list (ACL). For more information, see [Amazon S3 actions](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-with-s3-actions.html) and [Amazon S3 condition key examples](https://docs.aws.amazon.com/AmazonS3/latest/userguide/amazon-s3-policy-keys.html).
[Example Bucket policy](https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html)
```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "AddCannedAcl",
            "Effect": "Allow",
            "Principal": {
                "AWS": [
                    "arn:aws:iam::111122223333:root",
                    "arn:aws:iam::444455556666:root"
                ]
            },
            "Action": [
                "s3:PutObject",
                "s3:PutObjectAcl"
            ],
            "Resource": "arn:aws:s3:::DOC-EXAMPLE-BUCKET/*",
            "Condition": {
                "StringEquals": {
                    "s3:x-amz-acl": [
                        "public-read"
                    ]
                }
            }
        }
    ]
}
```

</TabItem>
</Tabs>




### HTTPS for encryption

:::caution
Among all 4 encryption types, only SSE-C requires all the requests need to through HTTPS.
:::

-   **SSE-C** - Amazon S3 will reject any requests made over HTTP.
-   **SSE-KMS** - It is not mandatory to use HTTPS.
-   **Client-Side Encryption** - Client-side encryption is the act of encrypting data before sending it to Amazon S3. It is not mandatory to use HTTPS for this.
-   **SSE-S3** - It is not mandatory to use HTTPS. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers.


### Force encryption in bucket policy

In order to ensure that all objects uploaded to a S3 bucket are encrypted, you can configure the bucket policy to deny if the `PutObject` does not have an `x-amz-server-side-encryption` header set.

- `'x-amz-server-side-encryption': 'AES256'` AES256 equals **SSE-S3**
  - Amazon S3 server-side encryption uses one of the strongest block ciphers available to encrypt your data, 256-bit Advanced Encryption Standard (AES-256).
- `'x-amz-server-side-encryption': 'aws:kms'`
  - Amazon S3 uses AWS KMS customer master keys (CMKs) to encrypt your Amazon S3 objects. AWS KMS encrypts only the object data. Any object metadata is not encrypted.	You can use if you need more control over your keys like create, rotating, disabling them using AWS KMS. Otherwise, if you wish to let AWS S3 manage your keys just stick with SSE-S3.


If you need server-side encryption for all of the objects that are stored in a bucket, use a bucket policy. For example, the following bucket policy denies permissions to upload an object unless the request includes the `x-amz-server-side-encryption` header to request server-side encryption:
```json
{
  "Version": "2012-10-17",
  "Id": "PutObjectPolicy",
  "Statement": [
    {
      "Sid": "DenyIncorrectEncryptionHeader",
      "Effect": "Deny",
      "Principal": "*",
      "Action": "s3:PutObject",
      "Resource": "arn:aws:s3:::awsexamplebucket1/*",
      "Condition": {
        "StringNotEquals": {
          "s3:x-amz-server-side-encryption": "AES256"
        }
      }
    },
    {
      "Sid": "DenyUnencryptedObjectUploads",
      "Effect": "Deny",
      "Principal": "*",
      "Action": "s3:PutObject",
      "Resource": "arn:aws:s3:::awsexamplebucket1/*",
      "Condition": {
        "Null": {
          "s3:x-amz-server-side-encryption": "true"
        }
      }
    }
  ]
}
```
> AWS document: [Protecting data using server-side encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html)


### Client Side Encryption

If you have a proprietary encryption algorithm to do the encryption, you have to leverage client-side encryption. To enable client-side encryption, you have the following options:
- Use a customer master key (CMK) stored in AWS Key Management Service (AWS KMS).
- Use a master key you store within your application.

### Generate CloudFron URL by Lambda

In general, if you’re using an Amazon **S3 bucket** as the origin for a **CloudFront distribution**, you can either 
- allow everyone to have access to the files there 
- or you can restrict access.

If you restrict access by using, for example, CloudFront signed URLs or signed cookies, you also won’t want people to be able to view files by simply using the direct **Amazon S3 URL** for the file. Instead, *you want them to only access the files by using the **CloudFront URL** *, so your content remains protected.


## Availabiltiy

### Multi-AZ

Multi-AZ for the RDS database helps make our database highly-available, but the standby database is not accessible and cannot be used for reads or write. It's just a database that will become primary when the other database encounters a failure.


## Limitation

### Request per sec
Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. Your applications can easily achieve thousands of transactions per second in request performance when uploading and retrieving storage from Amazon S3. Amazon S3 automatically scales to high request rates. 

For example, your application can achieve **at least 3,500 PUT/COPY/POST/DELETE or 5,500 GET requests per second per prefix in a bucket**.

There are no limits to the number of prefixes in a bucket. You can increase your read or write performance by parallelizing reads. For example, if you create **10 prefixes** in an Amazon S3 bucket to parallelize reads, you could scale your read performance to **55,000 read requests per second**(10*5,500 GET/HEAD).

For many purposes, it's often enough for applications to mitigate `503` SlowDown errors without having to randomize prefixes.

However, if the the limits are not sufficient, prefixes would need to be used. A prefix has no fixed number of characters. It is any string between a bucket name and an object name, for example:

- bucket/folder1/sub1/file
- bucket/folder1/sub2/file
- bucket/1/file
- bucket/2/file

Prefixes of the object 'file' would be: `/folder1/sub1/` , `/folder1/sub2/`, `/1/`, `/2/`. In this example, if you spread reads across all four prefixes evenly, you can achieve **22,000 requests per second.**

### Glacier Vault

You can't move data directly from Snowball into a **Glacier Vault** or a **Glacier Deep Archive Vault**. You need to go through S3 first and then use a lifecycle policy.

## Further reading

- [Best practices design patterns: optimizing Amazon S3 performance](https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html)
- [Stackoverflow: S3 - What Exactly Is A Prefix? And what Ratelimits apply?](https://stackoverflow.com/questions/52443839/s3-what-exactly-is-a-prefix-and-what-ratelimits-apply)